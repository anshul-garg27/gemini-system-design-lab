# Worker Service Improvement Guide

## üéØ Problem Solved

**Original Issue**: Database me duplicate rows ban rahe the kyunki `INSERT OR REPLACE` use kar rahe the aur title-based updates kar rahe the, lekin Gemini API title modify kar deta hai.

**Solution**: ID-based tracking system jo ensure karta hai ki:
- Har topic ka ek unique `topic_status_id` hai
- Worker iss ID ko processing ke dauran carry karta hai
- Updates title-based nahi, ID-based hote hain
- No duplicate rows created

## üìÅ Files Overview

### 1. **unified_database.py** (Updated)
Added three new methods:
```python
# Add topic and get its ID
topic_status_id = db.add_topic_for_processing("How Netflix Scales")

# Update by ID, not title
db.update_topic_status_by_id(topic_status_id, 'processing')

# Get pending topics WITH IDs
pending = db.get_pending_topics_with_ids(limit=10)  # Returns [(id, title), ...]
```

### 2. **app/improved_worker_service.py** (New)
Improved worker that:
- ‚úÖ Polls database for pending topics
- ‚úÖ Maintains ID throughout processing
- ‚úÖ Updates status by ID, not title
- ‚úÖ Handles Gemini's title modifications properly

### 3. **app/worker_service.py** (Original)
Keep this for backward compatibility, but migrate to improved version.

## üöÄ Usage

### Option 1: Run Improved Worker (Recommended)
```bash
# Default settings
python -m app.improved_worker_service

# With custom settings
WORKER_MAX_WORKERS=20 \
WORKER_BATCH_SIZE=5 \
WORKER_POLL_INTERVAL=10 \
python -m app.improved_worker_service
```

### Option 2: Add Topics via API
```python
# When topics are added via API/Frontend:
from unified_database import unified_db

# Add topic for processing (returns ID)
topic_status_id = unified_db.add_topic_for_processing("How WhatsApp Scales")

# Worker will automatically pick it up and process
```

### Option 3: Bulk Add Topics
```python
from unified_database import unified_db

topics = [
    "How Netflix CDN Works",
    "Instagram Photo Upload System",
    "WhatsApp Message Delivery"
]

for title in topics:
    topic_status_id = unified_db.add_topic_for_processing(title)
    print(f"Added: {title} with ID {topic_status_id}")
```

## üîÑ Workflow Comparison

### ‚ùå Old Workflow (Creates Duplicates)
```
1. Add topic: INSERT INTO topic_status (title='Netflix') 
2. Process: UPDATE topic_status SET status='processing' WHERE title='Netflix'
3. Gemini returns: "Enhanced: Netflix CDN Architecture"
4. Update: INSERT OR REPLACE INTO topic_status (title='Enhanced: Netflix CDN')
   ‚ö†Ô∏è Creates NEW row because title changed!
```

### ‚úÖ New Workflow (Maintains Consistency)
```
1. Add topic: INSERT INTO topic_status (title='Netflix') ‚Üí Returns ID=123
2. Process: UPDATE topic_status SET status='processing' WHERE id=123
3. Gemini returns: "Enhanced: Netflix CDN Architecture" 
4. Update: UPDATE topic_status SET status='completed' WHERE id=123
   ‚úÖ Same row updated, no duplicates!
```

## üìä Database Schema (Current)

### topic_status table
```sql
CREATE TABLE topic_status (
    id INTEGER PRIMARY KEY AUTOINCREMENT,  -- ‚úÖ Now we use this!
    title TEXT NOT NULL,                   -- Original title
    status TEXT NOT NULL,                  -- pending/processing/completed/failed
    error_message TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
```

### Key Changes:
- ‚úÖ Always use `id` for updates
- ‚úÖ Never use `INSERT OR REPLACE`
- ‚úÖ Use `UPDATE ... WHERE id = ?`

## üîç Monitoring

### Check Worker Status
```bash
# View logs
tail -f logs/worker.log

# Check database
sqlite3 unified.db "SELECT status, COUNT(*) FROM topic_status GROUP BY status"
```

### Check for Duplicates
```sql
-- Should return nothing if working correctly
SELECT title, COUNT(*) as count 
FROM topic_status 
GROUP BY title 
HAVING count > 1;
```

### View Processing Stats
```sql
SELECT 
    status,
    COUNT(*) as count,
    ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM topic_status), 2) as percentage
FROM topic_status
GROUP BY status;
```

## üêõ Debugging

### Problem: Topics stuck in 'processing'
```python
from unified_database import unified_db

# Reset stuck topics to pending
conn = unified_db.get_connection()
cursor = conn.cursor()
cursor.execute("""
    UPDATE topic_status 
    SET status = 'pending' 
    WHERE status = 'processing' 
    AND updated_at < datetime('now', '-1 hour')
""")
conn.commit()
conn.close()
```

### Problem: Duplicates still appearing
```sql
-- Check if old code is still running
SELECT * FROM topic_status 
WHERE title LIKE '%Enhanced:%' OR title LIKE '%Comprehensive:%'
ORDER BY created_at DESC LIMIT 20;

-- If duplicates found, you're using old worker_service.py
-- Switch to improved_worker_service.py
```

## üîÑ Migration Path

### Step 1: Test on Development
```bash
# Stop old worker
pkill -f worker_service.py

# Run improved worker
python -m app.improved_worker_service
```

### Step 2: Verify No Duplicates
```bash
# Add test topics
python -c "
from unified_database import unified_db
unified_db.add_topic_for_processing('Test Topic 1')
unified_db.add_topic_for_processing('Test Topic 2')
"

# Wait for processing
sleep 30

# Check for duplicates
sqlite3 unified.db "SELECT title, COUNT(*) FROM topic_status GROUP BY title HAVING COUNT(*) > 1"
```

### Step 3: Production Deployment
```bash
# Update supervisor/systemd config to use improved worker
# supervisord.conf:
[program:topic_worker]
command=/path/to/python -m app.improved_worker_service
directory=/path/to/project
autostart=true
autorestart=true
```

## üìù Code Integration Examples

### Example 1: Add Topic from FastAPI
```python
from fastapi import APIRouter
from unified_database import unified_db

router = APIRouter()

@router.post("/topics/add")
async def add_topic(title: str):
    topic_status_id = unified_db.add_topic_for_processing(title)
    
    return {
        "success": True,
        "topic_status_id": topic_status_id,
        "message": f"Topic added for processing with ID {topic_status_id}"
    }
```

### Example 2: Bulk Import from File
```python
import json
from unified_database import unified_db

def bulk_import_topics(filepath: str):
    with open(filepath) as f:
        topics = json.load(f)
    
    added_count = 0
    for topic in topics:
        title = topic.get('title')
        if title:
            topic_status_id = unified_db.add_topic_for_processing(title)
            if topic_status_id:
                added_count += 1
                print(f"‚úÖ Added: {title} (ID: {topic_status_id})")
    
    print(f"\nüìä Total added: {added_count}/{len(topics)}")

# Usage
bulk_import_topics('topics.json')
```

### Example 3: Retry Failed Topics
```python
from unified_database import unified_db

def retry_failed_topics():
    conn = unified_db.get_connection()
    cursor = conn.cursor()
    
    # Get failed topics
    cursor.execute("SELECT id FROM topic_status WHERE status = 'failed'")
    failed_ids = [row[0] for row in cursor.fetchall()]
    
    # Reset to pending
    for topic_id in failed_ids:
        unified_db.update_topic_status_by_id(topic_id, 'pending', error_message=None)
    
    conn.close()
    print(f"üîÑ Reset {len(failed_ids)} failed topics to pending")

# Usage
retry_failed_topics()
```

## ‚úÖ Benefits

1. **No Duplicates**: ID-based updates prevent duplicate rows
2. **Title Flexibility**: Gemini can modify titles without breaking tracking
3. **Easy Debugging**: Track topics by ID throughout lifecycle
4. **Consistency**: Same row updated from pending ‚Üí processing ‚Üí completed
5. **Scalability**: Multiple workers can process different topics safely
6. **Monitoring**: Clear status tracking per topic

## üéâ Summary

The improved worker service solves the consistency problem by:
- Using **ID-based tracking** instead of title-based
- Maintaining the **same row** throughout processing
- Handling **title modifications** from Gemini gracefully
- Working with your **existing polling mechanism**

No need to pass topics as arguments - just add them to `topic_status` table and the worker will automatically pick them up! üöÄ
